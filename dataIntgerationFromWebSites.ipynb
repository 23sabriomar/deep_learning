{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWCWh+XAqD6OCk1IFcs0PV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23sabriomar/deep_learning/blob/main/dataIntgerationFromWebSites.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3-Lt4AxDoLs",
        "outputId": "745911e7-fc8a-4e4d-f6d9-50d336b76623"
      },
      "source": [
        "!pip install requests --upgrade\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 446 kB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests) (2.0.6)\n",
            "Installing collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed requests-2.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfF6PGOyDtRn"
      },
      "source": [
        "import requests\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8YWw1OaD4ri"
      },
      "source": [
        "Nous pouvons utiliser requests.get pour télécharger une page . Ici, nous devons également définir des en-têtes dans cette fonction car la page Web de Google Scholar nécessite une connexion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aotYxCVpD6EB"
      },
      "source": [
        "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
        "url = 'https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=object+detection+in+aerial+image+&btnG=&oq=ob'\n",
        "response=requests.get(url,headers=headers)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r57a0wIEXkx"
      },
      "source": [
        "L'objet de réponse contient le résultat des requêtes, le code d'état et d'autres informations. Nous pouvons accéder au contenu de la page Web en utilisant response.text.\n",
        "\n",
        "***Analyser le code source du code HTML à l'aide d'une belle soupe***\n",
        "\n",
        "Nous utiliserons la bibliothèque Python Beautiful Soup pour analyser le code source HTML de la page Web téléchargée dans la section précédente. Installons et importons la bibliothèque Beautiful Soup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1PQQB5NEgUD",
        "outputId": "ffbb1f50-42be-4e1e-edad-cbd3d5480b59"
      },
      "source": [
        "!pip install beautifulsoup4\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQtjBQVpEyyN"
      },
      "source": [
        "Nous pouvons utiliser la BeautifulSoup classe pour analyser un document HTML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POzEM_Y4Epx_"
      },
      "source": [
        "from bs4 import BeautifulSoup\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpS6XIYNH3tl"
      },
      "source": [
        "doc = BeautifulSoup(response.content,'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3SwEJTnE1dR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omXZ2IPPGZGL"
      },
      "source": [
        "Une fois analysé, nous pouvons utiliser doc pour extraire des informations de la page Web. Maintenant, créons la fonction d'aide réutilisable get_paperinfo qui peut télécharger une page Web et créer un beau document de soupe pour une URL donnée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i92Y0SudGa12"
      },
      "source": [
        "# this function for the getting inforamtion of the web page\n",
        "def get_paperinfo(paper_url):\n",
        "\n",
        "  #download the page\n",
        "  response=requests.get(paper_url,headers=headers)\n",
        "\n",
        "  # check successful response\n",
        "  if response.status_code != 200:\n",
        "    print('Status code:', response.status_code)\n",
        "    raise Exception('Failed to fetch web page ')\n",
        "\n",
        "  #parse using beautiful soup\n",
        "  paper_doc = BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "  return paper_doc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qouo0p0UHISz"
      },
      "source": [
        "**Extraction de balises**\n",
        "\n",
        "Pour extraire les balises, nous allons créer une get_tags fonction qui renverra les balises pour le titre de l'article, la citation, l'URL et l'auteur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FslhfapLHDC2"
      },
      "source": [
        "# this function for the extracting information of the tags\n",
        "def get_tags(doc):\n",
        "  paper_tag = doc.select('[data-lid]')\n",
        "  cite_tag = doc.select('[title=Cite] + a')\n",
        "  link_tag = doc.find_all('h3',{\"class\" : \"gs_rt\"})\n",
        "  author_tag = doc.find_all(\"div\", {\"class\": \"gs_a\"})\n",
        "\n",
        "  return paper_tag,cite_tag,link_tag,author_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX1E0bJ-HSuG"
      },
      "source": [
        "**Pour extraire le titre de l' article de recherche** \n",
        "\n",
        "nous allons définir la fonction d' aide get_papertitle qui utilisera la liste des balises papier précédemment extraites ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF7CAKZ1HVDN"
      },
      "source": [
        "# it will return the title of the paper\n",
        "def get_papertitle(paper_tag):\n",
        "  \n",
        "  paper_names = []\n",
        "  \n",
        "  for tag in paper_tag:\n",
        "    paper_names.append(tag.select('h3')[0].get_text())\n",
        "\n",
        "  return paper_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDRDJjuGHlFt"
      },
      "source": [
        "**Extraction du nombre de citations de l'article**\n",
        "\n",
        "Maintenant, la get_citecount fonction d'assistance est définie pour extraire les informations du nombre de citations de l'article à l'aide de l'extrait ci-dessus cite_tag . Si l'article n'a pas de citation, le compte sera égal à zéro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZNVXnypHz1H"
      },
      "source": [
        "# it will return the number of citation of the paper\n",
        "def get_citecount(cite_tag):\n",
        "  cite_count = []\n",
        "  for i in cite_tag:\n",
        "    cite = i.text\n",
        "    if i is None or cite is None:  # if paper has no citatation then consider 0\n",
        "      cite_count.append(0)\n",
        "    else:\n",
        "      tmp = re.search(r'\\d+', cite) # its handle the None type object error and re use to remove the string \" cited by \" and return only integer value\n",
        "      if tmp is None :\n",
        "        cite_count.append(0)\n",
        "      else :\n",
        "        cite_count.append(int(tmp.group()))\n",
        "\n",
        "  return cite_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W3c2tftIABB"
      },
      "source": [
        "**Extraction de l'URL du document**\n",
        "\n",
        "La fonction d'assistance get_link renvoie l'URL de l'article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3rzLqXPIEgO"
      },
      "source": [
        "# function for the getting link information\n",
        "def get_link(link_tag):\n",
        "\n",
        "  links = []\n",
        "\n",
        "  for i in range(len(link_tag)) :\n",
        "    links.append(link_tag[i].a['href']) \n",
        "\n",
        "  return links "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYk97BMgIJVe"
      },
      "source": [
        "**Extraction du nom des auteurs, de l'année et des informations de publication**\n",
        "\n",
        "Pour extraire le nom des auteurs, l'année de publication de l'article et les informations de la conférence/journal de l'article dans lequel l'article a été publié, nous définirons la fonction get_author_year_publi_info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfqvw5ooIO8J"
      },
      "source": [
        "# function for the getting autho , year and publication information\n",
        "def get_author_year_publi_info(authors_tag):\n",
        "  years = []\n",
        "  publication = []\n",
        "  authors = []\n",
        "  for i in range(len(authors_tag)):\n",
        "      authortag_text = (authors_tag[i].text).split()\n",
        "      year = int(re.search(r'\\d+', authors_tag[i].text).group())\n",
        "      years.append(year)\n",
        "      publication.append(authortag_text[-1])\n",
        "      author = authortag_text[0] + ' ' + re.sub(',','', authortag_text[1])\n",
        "      authors.append(author)\n",
        "  \n",
        "  return years , publication, authors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvFYJ3zgISRq"
      },
      "source": [
        "**Compilez les données et créez un fichier CSV à l'aide de pandas**\n",
        "\n",
        "Maintenant, nous avons extrait toutes les informations concernant les documents de recherche de Google Scholar. Nous allons stocker ces informations sous forme de dictionnaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FRS5wVgIXB7"
      },
      "source": [
        "# creating final repository\n",
        "paper_repos_dict = {\n",
        "                    'Paper Title' : [],\n",
        "                    'Year' : [],\n",
        "                    'Author' : [],\n",
        "                    'Citation' : [],\n",
        "                    'Publication' : [],\n",
        "                    'Url of paper' : [] }\n",
        "\n",
        "# adding information in repository\n",
        "def add_in_paper_repo(papername,year,author,cite,publi,link):\n",
        "  paper_repos_dict['Paper Title'].extend(papername)\n",
        "  paper_repos_dict['Year'].extend(year)\n",
        "  paper_repos_dict['Author'].extend(author)\n",
        "  paper_repos_dict['Citation'].extend(cite)\n",
        "  paper_repos_dict['Publication'].extend(publi)\n",
        "  paper_repos_dict['Url of paper'].extend(link)\n",
        "\n",
        "  return pd.DataFrame(paper_repos_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438kn4fPIc6o"
      },
      "source": [
        "Dans le message ci-dessus, nous extrairons des informations de la page Web unique. Si nous voulons extraire des informations de plusieurs pages, nous parcourrons chaque page de Google Scholar. Ici, nous grattons les 10 pages de données au total. Nous pouvons gratter plus. Pour cela, nous devons changer le nombre dans la plage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFNrGGdiIfun"
      },
      "source": [
        "for i in range (0,110,10):\n",
        "\n",
        "  # get url for the each page\n",
        "  url = \"https://scholar.google.com/scholar?start={}&q=object+detection+in+aerial+image+&hl=en&as_sdt=0,5\".format(i)\n",
        "\n",
        "  # function for the get content of each page\n",
        "  doc = get_paperinfo(url)\n",
        "\n",
        "  # function for the collecting tags\n",
        "  paper_tag,cite_tag,link_tag,author_tag = get_tags(doc)\n",
        "  \n",
        "  # paper title from each page\n",
        "  papername = get_papertitle(paper_tag)\n",
        "\n",
        "  # year , author , publication of the paper\n",
        "  year , publication , author = get_author_year_publi_info(author_tag)\n",
        "\n",
        "  # cite count of the paper \n",
        "  cite = get_citecount(cite_tag)\n",
        "\n",
        "  # url of the paper\n",
        "  link = get_link(link_tag)\n",
        "\n",
        "  # add in paper repo dict\n",
        "  final = add_in_paper_repo(papername,year,author,cite,publication,link)\n",
        "  \n",
        "  # use sleep to avoid status code 429\n",
        "  sleep(30)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}